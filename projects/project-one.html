<!DOCTYPE html>
<html lang="en" class="scroll-smooth">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Adrian Krieger - Robotic Grinding Surface Quality</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap');
        body {
            font-family: 'Inter', sans-serif;
            background-color: #0b1120;
            color: #e2e8f0;
            background-image: url('https://images.unsplash.com/photo-1616429532585-f5b2b295c024?q=80&w=1740&auto=format&fit=crop');
            background-size: cover;
            background-position: center top;
            background-attachment: fixed;
            position: relative;
        }
        body::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background-color: rgba(0, 0, 0, 0.4);
            z-index: -1;
        }
        .nav-link:hover {
            color: #d1d5db;
        }
        .card {
            background-color: rgba(30, 41, 59, 0.7);
            backdrop-filter: blur(8px);
            box-shadow: 0 8px 16px -4px rgba(0, 0, 0, 0.4), 0 4px 8px -2px rgba(0, 0, 0, 0.2);
            transition: transform 0.2s ease-in-out;
            border: 1px solid rgba(148, 163, 184, 0.2);
        }
        .text-themed-blue { color: #a5b4fc; }
        .text-themed-green { color: #81e6d9; }
        .text-themed-purple { color: #c4b5fd; }
    </style>
</head>
<body class="antialiased">

    <header class="sticky top-0 bg-slate-900 bg-opacity-60 backdrop-blur-md shadow-lg z-50">
        <nav class="container mx-auto px-6 py-4 flex justify-between items-center">
            <a href="../index.html" class="text-2xl font-bold text-white">Adrian Krieger</a>
            <div class="hidden md:flex space-x-6 text-gray-300 font-medium">
                <a href="../index.html#about" class="nav-link">About</a>
                <a href="projects.html" class="nav-link">Projects</a>
                <a href="#" class="nav-link">Github</a>
                <a href="../index.html#cv-contact" class="nav-link">CV</a>
                <a href="../index.html#cv-contact" class="nav-link">Contact</a>
            </div>
            <button id="mobile-menu-button" class="md:hidden text-gray-300 focus:outline-none">
                <svg class="w-6 h-6" fill="none" stroke="currentColor" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16"></path>
                </svg>
            </button>
        </nav>
        <div id="mobile-menu" class="hidden md:hidden bg-slate-900 bg-opacity-60 backdrop-blur-md shadow-md">
            <div class="flex flex-col items-center py-4 space-y-2 text-gray-300 font-medium">
                <a href="../index.html#about" class="nav-link w-full text-center py-2 hover:bg-slate-800 transition-colors duration-200">About</a>
                <a href="projects.html" class="nav-link w-full text-center py-2 hover:bg-slate-800 transition-colors duration-200">Projects</a>
                <a href="#" class="nav-link w-full text-center py-2 hover:bg-slate-800 transition-colors duration-200">Github</a>
                <a href="../index.html#cv-contact" class="nav-link w-full text-center py-2 hover:bg-slate-800 transition-colors duration-200">CV</a>
                <a href="../index.html#cv-contact" class="nav-link w-full text-center py-2 hover:bg-slate-800 transition-colors duration-200">Contact</a>
            </div>
        </div>
    </header>

    <main class="container mx-auto px-6 py-8 md:py-16">
        <section id="project-one-details" class="py-12 md:py-20 text-center">
            <p class="text-lg font-medium text-themed-blue mb-2">Bachelor Thesis (2024)</p>
            <h1 class="text-4xl md:text-5xl font-bold text-white mb-6">Deep Learning for Automated Surface Quality Assessment in Robotic Grinding</h1>
            <p class="text-xl md:text-2xl text-gray-400 mb-12">Developing a robust dataset and evaluating machine learning viability for in-process quality control of metal components.</p>
            
            <div class="card p-8 rounded-xl max-w-5xl mx-auto text-left space-y-8">
                
                <div class="flex flex-wrap justify-center md:justify-start gap-4 mb-8">
                    <span class="px-3 py-1 bg-slate-800 text-themed-blue text-sm rounded-full font-medium">Deep Learning (DL)</span>
                    <span class="px-3 py-1 bg-slate-800 text-themed-blue text-sm rounded-full font-medium">Computer Vision</span>
                    <span class="px-3 py-1 bg-slate-800 text-themed-blue text-sm rounded-full font-medium">Robotics / EOAT</span>
                    <span class="px-3 py-1 bg-slate-800 text-themed-blue text-sm rounded-full font-medium">VGG16 & ResNet-18</span>
                </div>

                <div class="flex flex-wrap justify-center md:justify-start space-x-6 mb-8">
                    <a href="assets/documents/cv.pdf" target="_blank" class="text-themed-blue hover:text-blue-300 transition-colors duration-200 font-bold">Read Full Thesis (PDF) &rarr;</a>
                    <a href="#" class="text-gray-400 hover:text-gray-200 transition-colors duration-200 font-bold">View GitHub &rarr;</a>
                </div>
                
                <hr class="border-gray-700">

                <h2 class="text-3xl font-bold text-white pt-4">I. Introduction & Social Value</h2>
                <div class="text-lg text-gray-300 leading-relaxed space-y-4">
                    <p>
                        [cite_start]The final finishing step, such as **grinding**, in manufacturing processes is still overwhelmingly performed manually due to a lack of investment in automation[cite: 88]. [cite_start]Given the expected growth of AI and robotics in European industry [cite: 85] [cite_start]and the rising challenge of finding skilled workers[cite: 87], automating this final step is critical. [cite_start]The primary challenge lies in establishing **dynamic, in-process quality evaluation** that is superior to slow, contact-based tactile devices [cite: 94] [cite_start]or expensive laser systems[cite: 95].
                    </p>
                    <p>
                        [cite_start]The project's objective was to create a **preliminary dataset of labeled grinding images** using a camera system and experiment with different Machine Learning (ML) and Deep Learning (DL) approaches to establish a viable future direction for the **ROBOTe lab's fully automatic robotic-arm manufacturing solution**[cite: 115, 111]. [cite_start]The desired system needed the ability to determine if the necessary surface quality (mean roughness parameter Rz) had been reached[cite: 112, 113].
                    </p>
                </div>
                
                <div class="my-10 text-center">
                    <h3 class="text-xl font-semibold text-white mb-4">Workpiece Condition: Before & After Grinding</h3>
                                        [cite_start]<p class="text-sm text-gray-400 mt-2">The workpiece before grinding (left, showing milling grooves, Rz $\approx 12.7\mu m$) and after 124 iterations (right, Rz $\approx 1.1\mu m$). [cite: 141, 142]</p>
                </div>

                <hr class="border-gray-700">

                <h2 class="text-3xl font-bold text-white pt-4">II. Methods: Data Collection & Processing</h2>
                <div class="text-lg text-gray-300 leading-relaxed space-y-4">
                    <h3 class="text-xl font-semibold text-themed-blue mt-4 mb-2">A. Experimental Setup</h3>
                    <p>
                        [cite_start]Grinding was performed on a **C45 steel workpiece** using the **KUKA LBR iiwa 7 R800 robotic arm** and a prototype End-of-Arm-Tool (EOAT) to create realistic, real-world grinding patterns[cite: 131, 136, 116]. [cite_start]Images were acquired across 125 grinding iterations using three cameras (Canon EOS 50, Allied Vision, and Logitech C920) to compare accuracy-speed trade-offs[cite: 154].
                    </p>
                    <p>
                        To ensure model robustness, images were captured under varied conditions:
                        <ul class="list-disc list-inside space-y-1 pl-4 text-gray-300">
                            [cite_start]<li>**Angles:** Four 3D-printed brackets were used to simulate non-ideal factory scenarios by creating angles of $30^{\circ}$, $45^{\circ}$, $60^{\circ}$, and $90^{\circ}$ between the camera and the workpiece surface[cite: 119, 191].</li>
                            [cite_start]<li>**Lighting:** Seven unique lux configurations were used for $90^{\circ}$ images and four for angled images to increase robustness against variable factory lighting[cite: 264, 267]. [cite_start]Light-blocking material was added to reduce reflections and noise[cite: 294].</li>
                        </ul>
                    </p>
                    
                    <h3 class="text-xl font-semibold text-themed-blue mt-4 mb-2">B. Data Processing & Labeling</h3>
                    <p>
                        [cite_start]The dataset was labeled using the **highest value** from three roughness measurements taken at each location with a tactile device (Garant Perthometer H1)[cite: 362, 300]. [cite_start]This conservative approach ensures the model errs on the side of unnecessary reprocessing rather than producing a faulty part[cite: 365].
                    </p>
                    <p>
                        [cite_start]The raw images were pre-processed using a custom **edge detection algorithm** to automate background cropping, improving image alignment[cite: 659, 660]. [cite_start]The final dataset size was significantly increased (extended) by cropping each raw image into up to 24 smaller, specific regions based on where roughness was measured[cite: 673, 674, 722].
                    </p>
                    
                    <div class="my-10 text-center">
                        <h3 class="text-xl font-semibold text-white mb-4">Image Acquisition Setup</h3>
                                                [cite_start]<p class="text-sm text-gray-400 mt-2">Image acquisition setup showing the workpiece, LED bar lights, luxmeter, and camera positioning (Figure 3 in the thesis). [cite: 177, 166, 150]</p>
                    </div>
                </div>
                
                <hr class="border-gray-700">

                <h2 class="text-3xl font-bold text-white pt-4">III. Results & Evaluation</h2>
                <div class="text-lg text-gray-300 leading-relaxed space-y-4">
                    <p>
                        [cite_start]Experiments were conducted using Feed-Forward Neural Networks (FNN) as a baseline, and VGG16 and ResNet-18 Convolutional Neural Networks (CNN) for optimization[cite: 799]. [cite_start]The primary metric focused on **classification** accuracy (determining if grinding was complete)[cite: 795].
                    </p>
                    
                    <div class="my-10 text-center flex flex-col md:flex-row gap-4">
                        <div class="w-full md:w-1/2">
                            <h3 class="text-xl font-semibold text-white mb-4">Classification Performance (Baseline vs. Best)</h3>
                                                        <p class="text-sm text-gray-400 mt-2">Model 8 (Best) achieved significant improvements across all classes compared to the FNN baseline (Model 1).</p>
                        </div>
                        <div class="w-full md:w-1/2">
                            <h3 class="text-xl font-semibold text-white mb-4">ROC Curves Confirming Improvement</h3>
                                                        [cite_start]<p class="text-sm text-gray-400 mt-2">The ROC curve of Model 8 (right, average AUC $\approx 0.97$) shows superior performance to the baseline (left, average AUC $\approx 0.85$). [cite: 865, 866]</p>
                        </div>
                    </div>
                    
                    <h3 class="text-xl font-semibold text-themed-blue mt-4 mb-2">Key Findings:</h3>
                    <ul class="list-disc list-inside space-y-1 pl-4 text-gray-300">
                        [cite_start]<li>**Best Model:** The **VGG16 architecture** (Model 8), trained with $90^{\circ}$ images using optimized lux configurations (Lux 3-6) and **data augmentation**, achieved the highest test accuracy of **82.50%**[cite: 852, 1003].</li>
                        [cite_start]<li>**Regression Accuracy:** A VGG16 regression model achieved a Mean Absolute Error (**MAE**) of **$0.3328\mu m$** in the highest-precision $1\mu m-2\mu m$ roughness range[cite: 1004].</li>
                        [cite_start]<li>**Robustness:** The elimination of images taken under poor lighting (Lux 1, 2, and 7) and the use of augmentation led to an $18.64\%$ improvement over the baseline model[cite: 853, 855].</li>
                        [cite_start]<li>**Angled Performance:** The test accuracy for the $\mathbf{60^{\circ}}$ images was surprisingly high at **$74.00\%$** [cite: 946, 1011][cite_start], demonstrating that high-angle measurement is feasible where $90^{\circ}$ camera access is impossible[cite: 1012].</li>
                    </ul>
                </div>
                
                <hr class="border-gray-700">

                <h2 class="text-3xl font-bold text-white pt-4">IV. My Contribution & Next Steps</h2>
                <div class="text-lg text-gray-300 leading-relaxed space-y-4">
                    <p>
                        As the author of this Bachelor Thesis, I was the **sole researcher and implementer** responsible for the entire data acquisition and analysis process. My specific contributions include:
                    </p>
                    <ul class="list-disc list-inside space-y-1 pl-4 text-gray-300">
                        [cite_start]<li>**Experimental Design:** Conceptualizing the experimental setup, including designing the 3D-printed brackets to maintain a consistent workpiece center for all camera angles[cite: 191, 197].</li>
                        [cite_start]<li>**Data Collection:** Performing 125 grinding iterations using the KUKA robotic arm, acquiring the multi-angle, multi-lux image dataset, and conducting over 500 physical tactile roughness measurements[cite: 314, 514].</li>
                        [cite_start]<li>**Pipeline Development:** Writing all **Python scripts** for the data processing pipeline, including the automated background cropping via edge detection, generating the complex file naming convention (FNC), and creating the final classification and regression label files[cite: 660, 655, 745].</li>
                        [cite_start]<li>**Model Evaluation:** Running the VGG16 and ResNet-18 model training experiments and performing the detailed metric analysis (ROC, AUC, MAE) to determine the optimal configuration for in-process quality control[cite: 799, 819].</li>
                    </ul>
                </div>

                <div class="flex flex-wrap justify-center space-x-4 pt-8">
                    <a href="projects.html" class="text-gray-400 hover:text-gray-200 transition-colors duration-200 font-bold">‚Üê Back to Projects</a>
                </div>
            </div>
        </section>
    </main>

    <footer class="bg-slate-900 bg-opacity-60 backdrop-blur-md text-white text-center py-6">
        <p>&copy; 2025 Adrian Krieger. All rights reserved.</p>
    </footer>

    <script>
        // Smooth scrolling for navigation links
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                document.querySelector(this.getAttribute('href')).scrollIntoView({
                    behavior: 'smooth'
                });
                // Close mobile menu after clicking a link
                document.getElementById('mobile-menu').classList.add('hidden');
            });
        });

        // Toggle mobile menu visibility
        document.getElementById('mobile-menu-button').addEventListener('click', function() {
            const menu = document.getElementById('mobile-menu');
            menu.classList.toggle('hidden');
        });
    </script>
</body>
</html>
