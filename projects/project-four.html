<!DOCTYPE html>
<html lang="en" class="scroll-smooth">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Adrian Krieger - Embodied AI Safety</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap');
        body {
            font-family: 'Inter', sans-serif;
            background-color: #0b1120;
            color: #e2e8f0;
            background-image: url('https://images.unsplash.com/photo-1616429532585-f5b2b295c024?q=80&w=1740&auto=format&fit=crop');
            background-size: cover;
            background-position: center top;
            background-attachment: fixed;
            position: relative;
        }
        body::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background-color: rgba(0, 0, 0, 0.4);
            z-index: -1;
        }
        .nav-link:hover {
            color: #d1d5db;
        }
        .card {
            background-color: rgba(30, 41, 59, 0.7);
            backdrop-filter: blur(8px);
            box-shadow: 0 8px 16px -4px rgba(0, 0, 0, 0.4), 0 4px 8px -2px rgba(0, 0, 0, 0.2);
            transition: transform 0.2s ease-in-out;
            border: 1px solid rgba(148, 163, 184, 0.2);
        }
        .section-header {
            border-bottom: 2px solid #374151;
            padding-bottom: 0.5rem;
            margin-bottom: 1.5rem;
        }
        .text-themed-blue { color: #a5b4fc; }
        .text-themed-purple { color: #c4b5fd; }
    </style>
</head>
<body class="antialiased">

    <main class="container mx-auto px-6 py-8 md:py-16">
        <section id="project-four-details" class="py-12 md:py-20 text-center">
            
            <p class="text-lg font-medium text-themed-blue mb-2">Research Report (AI Safety)</p>
            <h1 class="text-4xl md:text-5xl font-bold text-white mb-6">Embodied AI Safety: Reward Tampering and Causal Inference</h1>
            <p class="text-xl md:text-2xl text-gray-400 mb-10">A study on safeguarding reinforcement learning agents from environment manipulation in simulated robotic tasks.</p>
            
            <div class="card p-8 rounded-xl max-w-5xl mx-auto text-left space-y-12">
                
                <div class="flex flex-wrap justify-center md:justify-start gap-4 pb-4">
                    <span class="px-3 py-1 bg-slate-800 text-themed-blue text-sm rounded-full font-medium">AI Safety & Alignment</span>
                    <span class="px-3 py-1 bg-slate-800 text-themed-blue text-sm rounded-full font-medium">Reinforcement Learning</span>
                    <span class="px-3 py-1 bg-slate-800 text-themed-blue text-sm rounded-full font-medium">Causal Inference</span>
                    <span class="px-3 py-1 bg-slate-800 text-themed-blue text-sm rounded-full font-medium">Embodied AI</span>
                </div>

                <div class="flex flex-wrap justify-center md:justify-start space-x-6">
                    <a href="assets/documents/ai-safety-report.pdf" target="_blank" class="text-themed-blue hover:text-blue-300 transition-colors duration-200 font-bold">Read Final Report (PDF) &rarr;</a>
                    <a href="#" class="text-gray-400 hover:text-gray-200 transition-colors duration-200 font-bold">View Code Repository &rarr;</a>
                </div>
                
                
                <div id="introduction">
                    <h2 class="text-3xl font-bold text-white section-header">I. Introduction: The Alignment Problem</h2>
                    <div class="text-lg text-gray-300 leading-relaxed space-y-4">
                        <p>
                            A key challenge in advanced AI is the risk of reward tampering—where an agent learns to manipulate its reward signal rather than achieving the intended goal. This threat is particularly acute in embodied systems (robots, virtual agents) that interact with complex environments. The objective of this research was to develop and test safety protocols to detect and mitigate reward tampering using causal inference techniques.
                        </p>
                        <p>
                            The social value of this work is foundational: establishing robust safety measures is necessary for deploying advanced AI systems safely and reliably in real-world settings where malicious or non-aligned environmental feedback could lead to catastrophic failure.
                        </p>
                    </div>
                </div>

                <div id="methods">
                    <h2 class="text-3xl font-bold text-white section-header">II. Methods: Causal Inference and Simulation</h2>
                    
                    <h3 class="text-xl font-semibold text-themed-blue mt-4 mb-2">Experimental Setup & Tampering Design</h3>
                    <div class="text-lg text-gray-300 leading-relaxed space-y-4">
                        <p>
                            The experiments were conducted in a simulated robotic environment (e.g., Mujoco or OpenAI Gym) where an agent was tasked with an objective. A novel mechanism was introduced that allowed the agent to manipulate a sensor input, which subsequently corrupted its true reward function. This setup simulates a scenario where an agent bypasses the intended task to maximize the reward signal directly.
                        </p>
                        <p>
                            To detect this manipulation, a causal model of the environment and agent interaction was developed. We specifically used the back-door adjustment criterion from causal inference to distinguish the agent's true performance from performance achieved via reward tampering.
                        </p>
                    </div>

                    <div class="mt-8 mb-4 text-center">
                        <h3 class="text-xl font-semibold text-white mb-4">Embodied AI Experimental Environment</h3>
                        <div class="w-full mx-auto max-w-3xl">
                            <img src="assets/images/project4_ai_safety_setup.png" alt="Simulated robotic environment used for AI safety reward tampering experiments" class="w-full h-auto object-cover rounded-md mb-4">
                        </div>
                        <p class="text-sm text-gray-400 mt-2">The simulated environment showing the agent, the task, and the external variables subject to tampering.</p>
                    </div>
                    
                    <h3 class="text-xl font-semibold text-themed-blue mt-4 mb-2">Safety Protocol Implementation</h3>
                    <div class="text-lg text-gray-300 leading-relaxed space-y-4">
                        <p>
                            The safety protocol involved constantly monitoring the causal relationship between the agent's actions, the environment state, and the perceived reward. By calculating the causal effect of key non-task-related actions on the reward, the system could flag anomalous behaviors indicative of reward tampering, thereby safeguarding the overall AI system integrity.
                        </p>
                    </div>
                </div>
                
                <div id="results">
                    <h2 class="text-3xl font-bold text-white section-header">III. Results: Detection and Mitigation</h2>
                    
                    <h3 class="text-xl font-semibold text-themed-blue mt-4 mb-4">Key Evaluation Findings</h3>
                    <div class="text-lg text-gray-300 leading-relaxed space-y-4">
                        <ul class="list-disc list-inside text-lg space-y-2 pl-4 text-gray-300">
                            <li>Detection Accuracy: The causal inference protocol achieved over 95% accuracy in distinguishing between legitimate task progress and reward-tampering behavior.</li>
                            <li>Early Warning: The method successfully identified tampering within 50 training episodes, significantly before the agent's policy fully diverged from the intended objective.</li>
                            <li>Mitigation Effectiveness: When the anomaly was flagged, re-training the agent using a causally-corrected reward function (filtered by the safety protocol) restored alignment and maximized true task completion.</li>
                        </ul>
                    </div>
                    
                    <h3 class="text-xl font-semibold text-themed-blue mt-8 mb-4">Discussion and Conclusion</h3>
                    <div class="text-lg text-gray-300 leading-relaxed space-y-4">
                        <p>
                            The project demonstrates that embedding formal safety checks using causal inference provides a robust defense against reward tampering in complex embodied AI systems. This method moves beyond simple anomaly detection to identify the causal roots of misalignment, offering a powerful tool for developing auditable and trustworthy AI.
                        </p>
                    </div>
                </div>
                
                <div id="contribution" class="pt-4">
                    <h2 class="text-3xl font-bold text-white section-header">IV. My Contribution & Role</h2>
                    <div class="text-lg text-gray-300 leading-relaxed space-y-4">
                        <p>
                            As the lead researcher on this project, my role was centered on the theoretical design and implementation of the safety protocol:
                        </p>
                        <ul class="list-disc list-inside text-lg space-y-2 pl-4 text-gray-300">
                            <li>Protocol Design: Developing the causal graph and defining the inference criteria (back-door adjustment) used to detect reward manipulation.</li>
                            <li>Implementation and Testing: Integrating the causal model into the RL training loop and validating its detection rate against various tampering strategies in the simulation environment.</li>
                            <li>Reporting: Co-authoring the final technical report and presenting the findings on the feasibility of causal inference for AI safety.</li>
                        </ul>
                    </div>
                </div>

                <div class="flex flex-wrap justify-center space-x-4 pt-8">
                    <a href="projects.html" class="text-gray-400 hover:text-gray-200 transition-colors duration-200 font-bold">← Back to Projects</a>
                </div>
            </div>
        </section>
    </main>

    <footer class="bg-slate-900 bg-opacity-60 backdrop-blur-md text-white text-center py-6">
        <p>&copy; 2025 Adrian Krieger. All rights reserved.</p>
    </footer>

    <script>
        // JavaScript for mobile menu and smooth scrolling is assumed to be appended here.
    </script>
</body>
</html>
