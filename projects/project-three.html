<!DOCTYPE html>
<html lang="en" class="scroll-smooth">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Adrian Krieger - OCR Optimization with RL</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap');
        body {
            font-family: 'Inter', sans-serif;
            background-color: #0b1120;
            color: #e2e8f0;
            background-image: url('https://images.unsplash.com/photo-1616429532585-f5b2b295c024?q=80&w=1740&auto=format&fit=crop');
            background-size: cover;
            background-position: center top;
            background-attachment: fixed;
            position: relative;
        }
        body::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background-color: rgba(0, 0, 0, 0.4);
            z-index: -1;
        }
        .nav-link:hover {
            color: #d1d5db;
        }
        .card {
            background-color: rgba(30, 41, 59, 0.7);
            backdrop-filter: blur(8px);
            box-shadow: 0 8px 16px -4px rgba(0, 0, 0, 0.4), 0 4px 8px -2px rgba(0, 0, 0, 0.2);
            transition: transform 0.2s ease-in-out;
            border: 1px solid rgba(148, 163, 184, 0.2);
        }
        .section-header {
            border-bottom: 2px solid #374151;
            padding-bottom: 0.5rem;
            margin-bottom: 1.5rem;
        }
        .text-themed-blue { color: #a5b4fc; }
        .text-themed-purple { color: #c4b5fd; }
    </style>
</head>
<body class="antialiased">

    <main class="container mx-auto px-6 py-8 md:py-16">
        <section id="project-three-details" class="py-12 md:py-20 text-center">
            
            <p class="text-lg font-medium text-themed-purple mb-2">Research Paper (Deep Learning & RL)</p>
            <h1 class="text-4xl md:text-5xl font-bold text-white mb-6">Optimizing OCR Pipelines using Reinforcement Learning</h1>
            <p class="text-xl md:text-2xl text-gray-400 mb-10">A deep reinforcement learning approach to automate document pre-processing and maximize recognition accuracy.</p>
            
            <div class="card p-8 rounded-xl max-w-5xl mx-auto text-left space-y-12">
                
                <div class="flex flex-wrap justify-center md:justify-start gap-4 pb-4">
                    <span class="px-3 py-1 bg-slate-800 text-themed-purple text-sm rounded-full font-medium">Reinforcement Learning (RL)</span>
                    <span class="px-3 py-1 bg-slate-800 text-themed-purple text-sm rounded-full font-medium">OCR Automation</span>
                    <span class="px-3 py-1 bg-slate-800 text-themed-purple text-sm rounded-full font-medium">Policy Gradient</span>
                    <span class="px-3 py-1 bg-slate-800 text-themed-purple text-sm rounded-full font-medium">Pytorch/Python</span>
                </div>

                <div class="flex flex-wrap justify-center md:justify-start space-x-6">
                    <a href="assets/documents/ocrl-paper.pdf" target="_blank" class="text-themed-purple hover:text-purple-300 transition-colors duration-200 font-bold">Read Full Paper (PDF) &rarr;</a>
                    <a href="#" class="text-gray-400 hover:text-gray-200 transition-colors duration-200 font-bold">View Code Repository &rarr;</a>
                </div>
                
                
                <div id="introduction">
                    <h2 class="text-3xl font-bold text-white section-header">I. Introduction: Problem & Social Value</h2>
                    <div class="text-lg text-gray-300 leading-relaxed space-y-4">
                        <p>
                            Traditional Optical Character Recognition (OCR) systems rely on fixed, sequential pre-processing steps (e.g., de-skew, noise reduction, thresholding). The optimal sequence of these steps varies drastically depending on the document's inherent noise, type, and quality, leading to inefficient manual iteration and sub-optimal final accuracy.
                        </p>
                        <p>
                            The objective of this research was to design and train a Reinforcement Learning (RL) agent to learn the optimal, dynamic sequence of pre-processing actions for any given input image. The social value lies in fully automating complex data entry and digitization tasks, significantly reducing human labor and computational overhead across industries reliant on legacy documents.
                        </p>
                    </div>
                </div>

                <div id="methods">
                    <h2 class="text-3xl font-bold text-white section-header">II. Methods: RL Architecture and Training</h2>
                    
                    <h3 class="text-xl font-semibold text-themed-purple mt-4 mb-2">System Architecture & Environment</h3>
                    <div class="text-lg text-gray-300 leading-relaxed space-y-4">
                        <p>
                            The system was built around an RL Agent trained to interact with a simulated environment representing the document pre-processing pipeline.
                        </p>
                        <ul class="list-disc list-inside space-y-2 pl-4 text-gray-300">
                            <li>Observation Space: Included the current image state (via CNN features) and the current recognition accuracy score.</li>
                            <li>Action Space: Consisted of a discrete set of common pre-processing operations (e.g., Gaussian Blur, morphological transformations, perspective correction).</li>
                            <li>Reward Function: Designed to reward the agent for maximizing the final OCR accuracy while penalizing it for using too many sequential actions (minimizing processing time).</li>
                        </ul>
                    </div>
                    
                    <div class="mt-8 mb-4 text-center">
                        <h3 class="text-xl font-semibold text-white mb-4">OCR Optimization System Architecture</h3>
                        <div class="w-full mx-auto max-w-3xl">
                             <img src="assets/images/project3_rl_architecture.png" alt="Diagram showing the deep reinforcement learning agent interacting with the OCR pre-processing environment" class="w-full h-auto object-cover rounded-md mb-4">
                        </div>
                        <p class="text-sm text-gray-400 mt-2">The architecture diagram showing the RL agent (Policy Network) optimizing the pre-processing steps.</p>
                    </div>
                    
                    <h3 class="text-xl font-semibold text-themed-purple mt-4 mb-2">Algorithm and Training</h3>
                    <div class="text-lg text-gray-300 leading-relaxed space-y-4">
                        <p>
                            A Policy Gradient method (specifically REINFORCE) was used as the core learning algorithm. The policy network itself was implemented using a Convolutional Neural Network (CNN) feature extractor coupled with a Recurrent Neural Network (RNN) to process the sequential nature of the actions taken. The model was trained against a diverse dataset of synthetically corrupted and real-world scanned documents.
                        </p>
                    </div>
                </div>
                
                <div id="results">
                    <h2 class="text-3xl font-bold text-white section-header">III. Results: Performance and Evaluation</h2>
                    
                    <h3 class="text-xl font-semibold text-themed-purple mt-4 mb-4">Performance Metrics</h3>
                    <div class="text-lg text-gray-300 leading-relaxed space-y-4">
                        <ul class="list-disc list-inside text-lg space-y-2 pl-4 text-gray-300">
                            <li>Accuracy Improvement: The RL-optimized pipeline achieved an average recognition accuracy improvement of 8.3% compared to the best traditional sequential pipeline baseline.</li>
                            <li>Efficiency: The agent demonstrated the ability to terminate the processing sequence earlier, resulting in a 20% reduction in the average number of actions required for high-quality documents.</li>
                            <li>Generalization: The trained agent successfully generalized to entirely new document types (e.g., handwritten notes vs. structured tables) without retraining, confirming its robustness.</li>
                        </ul>
                    </div>
                    
                    <h3 class="text-xl font-semibold text-themed-purple mt-8 mb-4">Discussion and Conclusion</h3>
                    <div class="text-lg text-gray-300 leading-relaxed space-y-4">
                        <p>
                            The project successfully demonstrated that Deep Reinforcement Learning can be effectively applied to control and optimize complex image processing pipelines where human expertise or fixed algorithms fail to generalize. The results confirm the method's feasibility for creating truly autonomous, adaptive OCR systems.
                        </p>
                    </div>
                </div>
                
                <div id="contribution" class="pt-4">
                    <h2 class="text-3xl font-bold text-white section-header">IV. My Contribution & Role</h2>
                    <div class="text-lg text-gray-300 leading-relaxed space-y-4">
                        <p>
                            My primary role was focused on the Reinforcement Learning design and implementation. My contributions included:
                        </p>
                        <ul class="list-disc list-inside text-lg space-y-2 pl-4 text-gray-300">
                            <li>RL Environment Development: Building the simulated environment that models the state transitions and reward calculations for the OCR pipeline.</li>
                            <li>Policy Network Implementation: Designing and coding the Policy Gradient agent (REINFORCE) using Pytorch, including the combined CNN-RNN architecture.</li>
                            <li>Training and Hyperparameter Optimization: Leading the training runs and tuning the reward function and network parameters to achieve optimal performance metrics.</li>
                            <li>Evaluation: Developing the statistical evaluation methods to compare the dynamic RL pipeline against traditional fixed-sequence baselines.</li>
                        </ul>
                    </div>
                </div>

                <div class="flex flex-wrap justify-center space-x-4 pt-8">
                    <a href="projects.html" class="text-gray-400 hover:text-gray-200 transition-colors duration-200 font-bold">‚Üê Back to Projects</a>
                </div>
            </div>
        </section>
    </main>

    <footer class="bg-slate-900 bg-opacity-60 backdrop-blur-md text-white text-center py-6">
        <p>&copy; 2025 Adrian Krieger. All rights reserved.</p>
    </footer>

    <script>
        // Smooth scrolling for navigation links
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                document.querySelector(this.getAttribute('href')).scrollIntoView({
                    behavior: 'smooth'
                });
                // Close mobile menu after clicking a link
                document.getElementById('mobile-menu').classList.add('hidden');
            });
        });

        // Toggle mobile menu visibility
        document.getElementById('mobile-menu-button').addEventListener('click', function() {
            const menu = document.getElementById('mobile-menu');
            menu.classList.toggle('hidden');
        });
    </script>
</body>
</html>
